# Real-Time Streaming Data Pipeline with Apache Kafka and Spark Streaming

A complete, production-ready real-time data processing pipeline that ingests user activity events using Apache Kafka and processes them with Apache Spark Streaming. The pipeline implements windowed aggregations, stateful transformations, and watermarking to handle late-arriving data.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Configuration](#configuration)
- [Running the Pipeline](#running-the-pipeline)
- [Project Structure](#project-structure)
- [Core Features](#core-features)
- [Testing and Verification](#testing-and-verification)
- [Troubleshooting](#troubleshooting)
- [Technical Details](#technical-details)

## ğŸ¯ Project Overview

This project demonstrates a complete real-time data pipeline architecture suitable for:

- **Real-time Analytics**: Process streaming events to generate live insights
- **Fraud Detection**: Monitor user behavior patterns in real-time
- **Live Monitoring**: Track system metrics and user activities as they happen
- **Data Lake**: Archive all events for historical analysis

### Key Technologies

- **Apache Kafka**: Distributed event streaming platform
- **Apache Spark Streaming**: Real-time data processing engine
- **PostgreSQL**: Relational database for real-time aggregations
- **Docker & Docker Compose**: Container orchestration
- **Parquet**: Columnar storage format for the data lake

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Activity Events                         â”‚
â”‚                  (Generated by producer.py)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Apache Kafka (user_activity) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Spark Streaming Application      â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚ Event Parsing & Schema Valid â”‚ â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚                                   â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  â”‚ 1. Tumbling Window (1 min)             â”‚
          â”‚  â”‚    â†’ Page View Counts                  â”‚
          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ 2. Sliding Window (5 min, 1 min slide) â”‚
          â”‚  â”‚    â†’ Active User Counts                â”‚
          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ 3. Stateful Transformation             â”‚
          â”‚  â”‚    â†’ User Session Duration             â”‚
          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ 4. Watermarking (2 min threshold)      â”‚
          â”‚  â”‚    â†’ Handle late-arriving data         â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚PostgreSQLâ”‚  â”‚ Parquet (Data   â”‚ â”‚Kafka Topic   â”‚
   â”‚ Database â”‚  â”‚ Lake)           â”‚ â”‚(enriched_    â”‚
   â”‚          â”‚  â”‚                 â”‚ â”‚activity)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Prerequisites

### System Requirements

- Docker (version 20.10+)
- Docker Compose (version 1.29+)
- Python 3.8+ (for running the producer locally if needed)
- At least 4GB RAM available for containers

### Ports Used

- **2181**: Zookeeper
- **9092**: Kafka (internal)
- **29092**: Kafka (external/localhost)
- **5432**: PostgreSQL
- **4040**: Spark UI (if exposed)

## ğŸš€ Setup Instructions

### 1. Clone and Prepare the Repository

```bash
git clone <repository-url>
cd pipeline
```

### 2. Create Environment File

Copy the example environment file and update values if needed:

```bash
cp .env.example .env
```

Default values in `.env`:

```
DB_USER=user
DB_PASSWORD=password
DB_NAME=stream_data
```

### 3. Build and Start Services

```bash
docker-compose up --build
```

This command will:

1. Build the Spark container
2. Start Zookeeper, Kafka, PostgreSQL, and Spark
3. Initialize PostgreSQL with the schema
4. Wait for all services to become healthy

**Expected output**: All containers should be in the "healthy" state within 2-3 minutes.

### 4. Verify All Services Are Running

```bash
docker-compose ps
```

All services should show status "Up" or "healthy".

## âš™ï¸ Configuration

### Environment Variables (.env file)

| Variable                  | Description              | Default     |
| ------------------------- | ------------------------ | ----------- |
| `DB_USER`                 | PostgreSQL username      | user        |
| `DB_PASSWORD`             | PostgreSQL password      | password    |
| `DB_NAME`                 | PostgreSQL database name | stream_data |
| `KAFKA_BOOTSTRAP_SERVERS` | Kafka bootstrap servers  | kafka:9092  |
| `SPARK_MEMORY_EXECUTOR`   | Spark executor memory    | 1g          |
| `SPARK_MEMORY_DRIVER`     | Spark driver memory      | 1g          |

### Kafka Configuration

- **Topic**: `user_activity` (for incoming events)
- **Partitions**: 1 (can be increased for load)
- **Replication Factor**: 1
- **Retention**: Default (7 days)

### Spark Configuration

Critical settings in the Spark application:

- **Parallelism**: 4 (adjust based on available cores)
- **Watermark**: 2 minutes (late data threshold)
- **Checkpoint Location**: `/tmp/spark-checkpoint`

## ğŸ”„ Running the Pipeline

### Step 1: Ensure All Services Are Running

```bash
docker-compose up -d
```

### Step 2: Start the Data Producer

Run the producer script to start generating events:

**Option A: Using Python directly (from your host machine)**

```bash
# Install dependencies if not already installed
pip install kafka-python

# Run the producer
python scripts/producer.py --bootstrap-servers localhost:29092 --interval 0.5
```

**Option B: Using Docker**

```bash
docker run -it --network pipeline_default \
  -e PYTHONUNBUFFERED=1 \
  python:3.9 \
  bash -c "pip install kafka-python && python scripts/producer.py --bootstrap-servers kafka:9092"
```

### Producer Script Options

```bash
python scripts/producer.py --help

Options:
  --bootstrap-servers SERVERS    Kafka bootstrap servers (default: localhost:29092)
  --topic TOPIC                  Kafka topic name (default: user_activity)
  --num-events NUM               Number of events to generate (default: 0 = infinite)
  --interval INTERVAL            Time between events in seconds (default: 0.5)
  --no-late-data                 Disable late-arriving data simulation
```

### Step 3: Monitor the Pipeline

**Check Spark Application Logs**:

```bash
docker-compose logs -f spark-app
```

**Check Database for Results**:

```bash
# Access PostgreSQL
docker exec -it db psql -U user -d stream_data

# Query results
SELECT * FROM page_view_counts LIMIT 10;
SELECT * FROM active_users LIMIT 10;
SELECT * FROM user_sessions LIMIT 10;
```

**Check Data Lake Files**:

```bash
ls -la ./data/lake/
# View Parquet files in subdirectories organized by event_date
```

**Monitor Kafka Topics**:

```bash
# Access Kafka container
docker exec kafka bash

# List topics
/usr/bin/kafka-topics --bootstrap-server localhost:9092 --list

# Consume from enriched_activity topic
/usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic enriched_activity --from-beginning
```

## ğŸ“ Project Structure

```
pipeline/
â”œâ”€â”€ docker-compose.yml          # Main orchestration file
â”œâ”€â”€ .env.example                # Environment variables template
â”œâ”€â”€ init-db.sql                 # PostgreSQL schema initialization
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ Dockerfile              # Spark application container
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ spark_streaming_app.py    # Main Spark Streaming application
â”‚       â””â”€â”€ db_utils.py                # Database utility functions
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ producer.py             # Kafka data producer
â”œâ”€â”€ data/
â”‚   â””â”€â”€ lake/                   # Data lake directory (Parquet files)
â””â”€â”€ README.md                   # This file
```

## âœ¨ Core Features

### 1. **Event Ingestion (Kafka)**

- Real-time event streaming from Kafka
- JSON event parsing with schema validation
- Support for late-arriving data

### 2. **Windowed Aggregations**

#### Tumbling Window (1 minute)

Calculates page view counts per URL in non-overlapping 1-minute windows.

```
Time:  0-60s    60-120s   120-180s
       â”Œâ”€â”€â”     â”Œâ”€â”€â”      â”Œâ”€â”€â”
Page 1 â”‚10â”‚     â”‚12â”‚      â”‚8 â”‚
       â””â”€â”€â”˜     â””â”€â”€â”˜      â””â”€â”€â”˜
```

#### Sliding Window (5 minutes, 1 minute slide)

Counts distinct active users in overlapping 5-minute windows, advancing every 1 minute.

```
Time:     0-300s (Window 1)
             â””â”€60-360s (Window 2)
                â””â”€120-420s (Window 3)
```

### 3. **Stateful Transformation**

Tracks user sessions from `session_start` to `session_end` events, calculating duration. Features:

- Automatic timeout after 15 minutes of inactivity
- Idempotent writes to prevent duplicates
- State recovery from checkpoint store

### 4. **Watermarking**

Implements a 2-minute watermark threshold:

- Events arriving more than 2 minutes late are dropped
- Prevents cascading updates to historical aggregations
- Example: Event with timestamp from 3 minutes ago â†’ dropped

### 5. **Multiple Sinks**

| Sink       | Purpose                | Format     | Partitioning  |
| ---------- | ---------------------- | ---------- | ------------- |
| PostgreSQL | Real-time aggregations | SQL tables | None          |
| Data Lake  | Historical archive     | Parquet    | By event_date |
| Kafka      | Downstream processing  | JSON       | By key        |

### 6. **Exactly-Once Semantics**

- Idempotent database writes using `INSERT ... ON CONFLICT DO UPDATE`
- Checkpoint management for fault recovery
- Transactional Kafka producers

## ğŸ§ª Testing and Verification

### Test 1: Basic Event Flow

```bash
# Terminal 1: Check if events are reaching Kafka
docker exec kafka /usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic user_activity --from-beginning --max-messages 5

# Terminal 2: Check enriched events
docker exec kafka /usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic enriched_activity --from-beginning --max-messages 5
```

### Test 2: Tumbling Window Aggregation

```bash
# After 2-3 minutes of producing events:
docker exec db psql -U user -d stream_data -c \
  "SELECT * FROM page_view_counts ORDER BY window_start DESC LIMIT 5;"

# Expected: Records with window_end - window_start = 60 seconds
```

### Test 3: Sliding Window Aggregation

```bash
docker exec db psql -U user -d stream_data -c \
  "SELECT window_start, window_end, active_user_count FROM active_users
   ORDER BY window_start DESC LIMIT 10;"

# Expected: Window duration = 5 minutes, different windows with 1-minute increments
```

### Test 4: Stateful Transformation

Generate session events manually:

```bash
# In another terminal, modify the producer to send session events:
# Edit scripts/producer.py to increase session_start/session_end frequency
python scripts/producer.py --interval 2.0

# Check sessions:
docker exec db psql -U user -d stream_data -c \
  "SELECT user_id, session_duration_seconds FROM user_sessions LIMIT 10;"
```

### Test 5: Watermarking and Late Data

The producer script automatically sends late events every 50 events:

```bash
# Monitor logs to see watermarking behavior:
docker-compose logs spark-app | grep -i "watermark\|late"

# Late events should be dropped (not appear in aggregations)
```

### Test 6: Data Lake Verification

```bash
# Check Parquet files exist
find ./data/lake -name "*.parquet" | head -5

# Read a Parquet file (if you have Parquet tools installed)
# Or use Python:
python -c "
import pandas as pd
import pyarrow.parquet as pq
files = !find ./data/lake -name '*.parquet' -type f
if files:
    table = pq.read_table(files[0])
    print(table.to_pandas().head())
"
```

## ğŸ”§ Troubleshooting

### Issue: "Connection refused" when connecting to Kafka

**Solution**: Ensure Kafka is healthy:

```bash
docker-compose logs kafka
docker-compose ps kafka
# Wait for "healthy" status
```

### Issue: PostgreSQL container won't start

**Solution**: Check database logs and ensure port 5432 is free:

```bash
docker-compose logs db
netstat -an | grep 5432
```

### Issue: Spark app crashes immediately

**Solution**: Check for missing dependencies:

```bash
docker-compose logs spark-app
# Look for: ImportError, ModuleNotFoundError
```

**Fix**: Rebuild the container:

```bash
docker-compose down
docker-compose build --no-cache
docker-compose up
```

### Issue: No data appears in database tables

**Possible causes**:

1. Producer script not running or sending events
2. Kafka topic doesn't exist
3. Spark not connected to Kafka

**Debug**:

```bash
# Check topic exists
docker exec kafka /usr/bin/kafka-topics --bootstrap-server localhost:9092 --list

# Check events in topic
docker exec kafka /usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic user_activity --max-messages 1

# Check Spark logs
docker-compose logs spark-app | tail -50
```

### Issue: Out of memory errors

**Solution**: Increase Docker memory or reduce Spark parallelism:

1. Increase Docker memory limits in `docker-compose.yml`
2. Reduce `spark.sql.shuffle.partitions` in `spark_streaming_app.py`

### Issue: Data not being partitioned correctly in data lake

**Solution**: Ensure `event_date` column is added properly in Spark app:

```python
# The spark_streaming_app.py already includes:
parsed_df = parsed_df.withColumn(
    'event_date',
    col('event_time').cast('date')
)
```

## ğŸ“Š Technical Details

### Event Schema

```json
{
  "event_time": "2024-01-28T10:30:00Z",
  "user_id": "user_5",
  "page_url": "https://example.com/products",
  "event_type": "page_view|click|session_start|session_end"
}
```

### Database Schema

#### page_view_counts

```sql
- window_start: TIMESTAMP
- window_end: TIMESTAMP
- page_url: TEXT
- view_count: BIGINT
- PRIMARY KEY: (window_start, window_end, page_url)
```

#### active_users

```sql
- window_start: TIMESTAMP
- window_end: TIMESTAMP
- active_user_count: BIGINT
- PRIMARY KEY: (window_start, window_end)
```

#### user_sessions

```sql
- user_id: TEXT (PRIMARY KEY)
- session_start_time: TIMESTAMP
- session_end_time: TIMESTAMP
- session_duration_seconds: BIGINT
```

### Checkpoint and State Management

Spark checkpointing directories:

- `/tmp/spark-checkpoint` - Main application state
- `/tmp/checkpoint-data-lake` - Data lake sink state
- `/tmp/checkpoint-enriched-kafka` - Enriched Kafka sink state
- `/tmp/checkpoint-page-views` - Page views aggregation state
- `/tmp/checkpoint-active-users` - Active users aggregation state
- `/tmp/checkpoint-sessions` - Sessions transformation state

### Performance Considerations

1. **Parallelism**: Currently set to 4. Increase for larger datasets:

   ```python
   .config('spark.sql.shuffle.partitions', '8')
   ```

2. **Memory**: Default 1GB per executor. Adjust in `.env`:

   ```
   SPARK_MEMORY_EXECUTOR=2g
   SPARK_MEMORY_DRIVER=2g
   ```

3. **Batch Interval**: Consider increasing producer interval for high-volume scenarios:

   ```bash
   python scripts/producer.py --interval 0.1  # 100ms between events
   ```

4. **Data Lake Compression**: Parquet files are already compressed. To change:
   ```python
   .option('compression', 'snappy')  # or 'gzip', 'uncompressed'
   ```

### Production Deployment Considerations

1. **Schema Registry**: Use Confluent Schema Registry instead of inline schema
2. **Monitoring**: Integrate with Prometheus/Grafana for metrics
3. **Alerting**: Set up alerts for late data, failed writes, checkpoint lag
4. **Backup**: Regular backups of PostgreSQL data and checkpoint stores
5. **Scaling**: Use Kafka partitioning and Spark dynamic allocation
6. **Security**: Enable authentication for Kafka, PostgreSQL, and add TLS

## ğŸ“ License

This project is provided as-is for educational purposes.

## ğŸ¤ Contributing

For improvements or bug reports, please submit issues or pull requests.

---

**Last Updated**: January 2026
**Spark Version**: 3.3.2
**Kafka Version**: 7.3.0
**PostgreSQL Version**: 14
