# Real-Time Data Pipeline using Apache Kafka and Spark

This is a project I built to learn about real-time data engineering. It focuses on ingesting user activity events (like clicks and page views) from a website and processing them on the fly using Spark Streaming. 

The goal was to implement key concepts like time-based windows, stateful sessions, and handling late-arriving data in a distributed environment.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Configuration](#configuration)
- [Running the Pipeline](#running-the-pipeline)
- [Project Structure](#project-structure)
- [Core Features](#core-features)
- [Testing and Verification](#testing-and-verification)
- [Troubleshooting](#troubleshooting)
- [Technical Details](#technical-details)

## ğŸ’¡ Why this project?

Real-time processing is a huge part of modern data engineering. I wanted to build an end-to-end flow to understand:
- How Kafka manages streaming event topics.
- How Spark handles stateful operations (like tracking a user's session time).
- How to make sure no data is lost even if a service restarts.

### Components Used
- **Apache Kafka**: For the initial event ingestion.
- **Spark Structured Streaming**: The core engine for processing logic.
- **PostgreSQL**: Where the final analytics are stored.
- **Docker**: To keep all these services easy to ship and run anywhere.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Activity Events                         â”‚
â”‚                  (Generated by producer.py)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Apache Kafka (user_activity) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Spark Streaming Application      â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚ Event Parsing & Schema Valid â”‚ â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚                                   â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  â”‚ 1. Tumbling Window (1 min)             â”‚
          â”‚  â”‚    â†’ Page View Counts                  â”‚
          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ 2. Sliding Window (5 min, 1 min slide) â”‚
          â”‚  â”‚    â†’ Active User Counts                â”‚
          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ 3. Stateful Transformation             â”‚
          â”‚  â”‚    â†’ User Session Duration             â”‚
          â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  â”‚ 4. Watermarking (2 min threshold)      â”‚
          â”‚  â”‚    â†’ Handle late-arriving data         â”‚
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚PostgreSQLâ”‚  â”‚ Parquet (Data   â”‚ â”‚Kafka Topic   â”‚
   â”‚ Database â”‚  â”‚ Lake)           â”‚ â”‚(enriched_    â”‚
   â”‚          â”‚  â”‚                 â”‚ â”‚activity)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Prerequisites

### System Requirements

- Docker (version 20.10+)
- Docker Compose (version 1.29+)
- Python 3.8+ (for running the producer locally if needed)
- At least 4GB RAM available for containers

### Ports Used

- **2181**: Zookeeper
- **9092**: Kafka (internal)
- **29092**: Kafka (external/localhost)
- **5432**: PostgreSQL
- **4040**: Spark UI (if exposed)

## ğŸš€ Setup Instructions

### 1. Clone and Prepare the Repository

```bash
git clone <repository-url>
cd pipeline
```

### 2. Create Environment File

Copy the example environment file and update values if needed:

```bash
cp .env.example .env
```

Default values in `.env`:

```
DB_USER=user
DB_PASSWORD=password
DB_NAME=stream_data
```

### 3. Build and Start Services

```bash
docker-compose up --build
```

This command will:

1. Build the Spark container
2. Start Zookeeper, Kafka, PostgreSQL, and Spark
3. Initialize PostgreSQL with the schema
4. Wait for all services to become healthy

**Expected output**: All containers should be in the "healthy" state within 2-3 minutes.

### 4. Verify All Services Are Running

```bash
docker-compose ps
```

All services should show status "Up" or "healthy".

## âš™ï¸ Configuration

### Environment Variables (.env file)

| Variable                  | Description              | Default     |
| ------------------------- | ------------------------ | ----------- |
| `DB_USER`                 | PostgreSQL username      | user        |
| `DB_PASSWORD`             | PostgreSQL password      | password    |
| `DB_NAME`                 | PostgreSQL database name | stream_data |
| `KAFKA_BOOTSTRAP_SERVERS` | Kafka bootstrap servers  | kafka:9092  |
| `SPARK_MEMORY_EXECUTOR`   | Spark executor memory    | 1g          |
| `SPARK_MEMORY_DRIVER`     | Spark driver memory      | 1g          |

### Kafka Configuration

- **Topic**: `user_activity` (for incoming events)
- **Partitions**: 1 (can be increased for load)
- **Replication Factor**: 1
- **Retention**: Default (7 days)

### Spark Configuration

Critical settings in the Spark application:

- **Parallelism**: 4 (adjust based on available cores)
- **Watermark**: 2 minutes (late data threshold)
- **Checkpoint Location**: `/tmp/spark-checkpoint`

## ğŸ”„ Running the Pipeline

### Step 1: Ensure All Services Are Running

```bash
docker-compose up -d
```

### Step 2: Start the Data Producer

Run the producer script to start generating events:

**Option A: Using Python directly (from your host machine)**

```bash
# Install dependencies if not already installed
pip install kafka-python

# Run the producer
python scripts/producer.py --bootstrap-servers localhost:29092 --interval 0.5
```

**Option B: Using Docker**

```bash
docker run -it --network pipeline_default \
  -e PYTHONUNBUFFERED=1 \
  python:3.9 \
  bash -c "pip install kafka-python && python scripts/producer.py --bootstrap-servers kafka:9092"
```

### Producer Script Options

```bash
python scripts/producer.py --help

Options:
  --bootstrap-servers SERVERS    Kafka bootstrap servers (default: localhost:29092)
  --topic TOPIC                  Kafka topic name (default: user_activity)
  --num-events NUM               Number of events to generate (default: 0 = infinite)
  --interval INTERVAL            Time between events in seconds (default: 0.5)
  --no-late-data                 Disable late-arriving data simulation
```

### Step 3: Monitor the Pipeline

**Check Spark Application Logs**:

```bash
docker-compose logs -f spark-app
```

**Check Database for Results**:

```bash
# Access PostgreSQL
docker exec -it db psql -U user -d stream_data

# Query results
SELECT * FROM page_view_counts LIMIT 10;
SELECT * FROM active_users LIMIT 10;
SELECT * FROM user_sessions LIMIT 10;
```

**Check Data Lake Files**:

```bash
ls -la ./data/lake/
# View Parquet files in subdirectories organized by event_date
```

**Monitor Kafka Topics**:

```bash
# Access Kafka container
docker exec kafka bash

# List topics
/usr/bin/kafka-topics --bootstrap-server localhost:9092 --list

# Consume from enriched_activity topic
/usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic enriched_activity --from-beginning
```

## ğŸ“ Project Structure

```
pipeline/
â”œâ”€â”€ docker-compose.yml          # Main orchestration file
â”œâ”€â”€ .env.example                # Environment variables template
â”œâ”€â”€ init-db.sql                 # PostgreSQL schema initialization
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ Dockerfile              # Spark application container
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ spark_streaming_app.py    # Main Spark Streaming application
â”‚       â””â”€â”€ db_utils.py                # Database utility functions
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ producer.py             # Kafka data producer
â”œâ”€â”€ data/
â”‚   â””â”€â”€ lake/                   # Data lake directory (Parquet files)
â””â”€â”€ README.md                   # This file
```

## ğŸ§  Core Implementation Details

### 1. Ingestion Logic
I used a Python producer to simulate real-time traffic. The Spark app reads this JSON data from the `user_activity` topic and converts it into a structured DataFrame.

### 2. Time-Based Windows
I implemented two types of windows to see the difference in behavior:
- **Tumbling Window (1 min)**: For simple page view counts.
- **Sliding Window (5 min duration, 1 min slide)**: To track active users over a rolling period.

### 3. Session Tracking (Stateful)
This was the most challenging part. It tracks when a user starts and ends a session. If a user is inactive for 15 minutes, Spark automatically "times out" the session using a stateful transformation.

### 4. Handling Late Data (Watermarking)
Data sometimes arrives late due to network lag. I set a **2-minute watermark**â€”this tells Spark to wait up to 2 minutes for late events before finalizing the window results.

### 5. Multi-Sink Strategy
One stream is processed and Sent to three different places:
1. **PostgreSQL**: For real-time updates (using "UPSERT" logic to prevent duplicates).
2. **Data Lake**: Raw events saved in Parquet format, partitioned by date.
3. **Kafka**: Enriched events (adding a processing timestamp) sent to a new topic.

## ğŸ§ª Testing and Verification

### Test 1: Basic Event Flow

```bash
# Terminal 1: Check if events are reaching Kafka
docker exec kafka /usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic user_activity --from-beginning --max-messages 5

# Terminal 2: Check enriched events
docker exec kafka /usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic enriched_activity --from-beginning --max-messages 5
```

### Test 2: Tumbling Window Aggregation

```bash
# After 2-3 minutes of producing events:
docker exec db psql -U user -d stream_data -c \
  "SELECT * FROM page_view_counts ORDER BY window_start DESC LIMIT 5;"

# Expected: Records with window_end - window_start = 60 seconds
```

### Test 3: Sliding Window Aggregation

```bash
docker exec db psql -U user -d stream_data -c \
  "SELECT window_start, window_end, active_user_count FROM active_users
   ORDER BY window_start DESC LIMIT 10;"

# Expected: Window duration = 5 minutes, different windows with 1-minute increments
```

### Test 4: Stateful Transformation

Generate session events manually:

```bash
# In another terminal, modify the producer to send session events:
# Edit scripts/producer.py to increase session_start/session_end frequency
python scripts/producer.py --interval 2.0

# Check sessions:
docker exec db psql -U user -d stream_data -c \
  "SELECT user_id, session_duration_seconds FROM user_sessions LIMIT 10;"
```

### Test 5: Watermarking and Late Data

The producer script automatically sends late events every 50 events:

```bash
# Monitor logs to see watermarking behavior:
docker-compose logs spark-app | grep -i "watermark\|late"

# Late events should be dropped (not appear in aggregations)
```

### Test 6: Data Lake Verification

```bash
# Check Parquet files exist
find ./data/lake -name "*.parquet" | head -5

# Read a Parquet file (if you have Parquet tools installed)
# Or use Python:
python -c "
import pandas as pd
import pyarrow.parquet as pq
files = !find ./data/lake -name '*.parquet' -type f
if files:
    table = pq.read_table(files[0])
    print(table.to_pandas().head())
"
```

## ğŸ”§ Troubleshooting

### Issue: "Connection refused" when connecting to Kafka

**Solution**: Ensure Kafka is healthy:

```bash
docker-compose logs kafka
docker-compose ps kafka
# Wait for "healthy" status
```

### Issue: PostgreSQL container won't start

**Solution**: Check database logs and ensure port 5432 is free:

```bash
docker-compose logs db
netstat -an | grep 5432
```

### Issue: Spark app crashes immediately

**Solution**: Check for missing dependencies:

```bash
docker-compose logs spark-app
# Look for: ImportError, ModuleNotFoundError
```

**Fix**: Rebuild the container:

```bash
docker-compose down
docker-compose build --no-cache
docker-compose up
```

### Issue: No data appears in database tables

**Possible causes**:

1. Producer script not running or sending events
2. Kafka topic doesn't exist
3. Spark not connected to Kafka

**Debug**:

```bash
# Check topic exists
docker exec kafka /usr/bin/kafka-topics --bootstrap-server localhost:9092 --list

# Check events in topic
docker exec kafka /usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic user_activity --max-messages 1

# Check Spark logs
docker-compose logs spark-app | tail -50
```

### Issue: Out of memory errors

**Solution**: Increase Docker memory or reduce Spark parallelism:

1. Increase Docker memory limits in `docker-compose.yml`
2. Reduce `spark.sql.shuffle.partitions` in `spark_streaming_app.py`

### Issue: Data not being partitioned correctly in data lake

**Solution**: Ensure `event_date` column is added properly in Spark app:

```python
# The spark_streaming_app.py already includes:
parsed_df = parsed_df.withColumn(
    'event_date',
    col('event_time').cast('date')
)
```

## ğŸ“Š Technical Details

### Event Schema

```json
{
  "event_time": "2024-01-28T10:30:00Z",
  "user_id": "user_5",
  "page_url": "https://example.com/products",
  "event_type": "page_view|click|session_start|session_end"
}
```

### Database Schema

#### page_view_counts

```sql
- window_start: TIMESTAMP
- window_end: TIMESTAMP
- page_url: TEXT
- view_count: BIGINT
- PRIMARY KEY: (window_start, window_end, page_url)
```

#### active_users

```sql
- window_start: TIMESTAMP
- window_end: TIMESTAMP
- active_user_count: BIGINT
- PRIMARY KEY: (window_start, window_end)
```

#### user_sessions

```sql
- user_id: TEXT (PRIMARY KEY)
- session_start_time: TIMESTAMP
- session_end_time: TIMESTAMP
- session_duration_seconds: BIGINT
```

### Checkpoint and State Management

Spark checkpointing directories:

- `/tmp/spark-checkpoint` - Main application state
- `/tmp/checkpoint-data-lake` - Data lake sink state
- `/tmp/checkpoint-enriched-kafka` - Enriched Kafka sink state
- `/tmp/checkpoint-page-views` - Page views aggregation state
- `/tmp/checkpoint-active-users` - Active users aggregation state
- `/tmp/checkpoint-sessions` - Sessions transformation state

### Performance Considerations

1. **Parallelism**: Currently set to 4. Increase for larger datasets:

   ```python
   .config('spark.sql.shuffle.partitions', '8')
   ```

2. **Memory**: Default 1GB per executor. Adjust in `.env`:

   ```
   SPARK_MEMORY_EXECUTOR=2g
   SPARK_MEMORY_DRIVER=2g
   ```

3. **Batch Interval**: Consider increasing producer interval for high-volume scenarios:

   ```bash
   python scripts/producer.py --interval 0.1  # 100ms between events
   ```

4. **Data Lake Compression**: Parquet files are already compressed. To change:
   ```python
   .option('compression', 'snappy')  # or 'gzip', 'uncompressed'
   ```

## ğŸš€ Future Improvements

If I were to take this project further, I would:
1. **Add a Schema Registry**: To manage Avro/Protobuf schemas instead of raw JSON.
2. **Set up Monitoring**: Use Prometheus and Grafana to visualize the Spark stream lag.
3. **Security**: Add TLS and SASL authentication to the Kafka cluster.
4. **CI/CD**: Add GitHub Actions to auto-test the Spark logic on push.

## ğŸ“ License

This project is provided as-is for educational purposes.

## ğŸ¤ Contributing

For improvements or bug reports, please submit issues or pull requests.

---

**Last Updated**: January 2026
**Spark Version**: 3.3.2
**Kafka Version**: 7.3.0
**PostgreSQL Version**: 14
